[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Online trend estimation and detection of deviations",
    "section": "",
    "text": "Paper: Online trend estimation and detection of trend deviations in sub-sewershed time series of SARS-CoV-2 RNA measured in wastewater\n\n\n\n\n\nKatherine B. Ensor, Julia Schedler, Thomas Sun, Rebecca Schneider, Anthony Mulenga, Jingjing Wu, Lauren B. Stadler, Loren Hopkins\nmedRxiv 2023.10.26.23297635; doi: https://doi.org/10.1101/2023.10.26.23297635\n\n\n\nThis website provides details on the application of the state-space modeling and statistical process control frameworks to the time series of Sars-cov2 viral load for various sampling sites in the City of Houston.\nThe purpose of this analysis is to develop a method that\n\ncompares series from two different sampling sites\naccounts for measurement error\nis comparable to the existing B-spline method employed by the City.\n\nIntended audience: those who wish to replicate the analyses demonstrated here on their own WW epi data.\n\n\n\n\n\n\nAssumed knowledge + resources to fill gaps\n\n\n\n\n\n\nability to interpret algebraic equations\nbasic familiarity with R programming\n\nSuggested external learning resource: R for Data Science 2E by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund.\n\nan understanding of linear regression\n\nSuggested external learning resource: Chapter 7 of Introduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin.\n\na willingness to learn some basic time series techniques.\n\nSuggested external learning resource: Time Series: A Data Analysis Approach Using R by Robert Shumway and David Stoffer.\nSuggested external learning resource: Forecasting: Principles and Practice 3E by Rob J Hyndman and George Athanasopoulos.\n\n\n\n\n\n\n\n\n\n\n\nContact us!\n\n\n\n\nhttps://hou-wastewater-epi.org/contact-us",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step1",
    "href": "Algorithm1.html#sec-step1",
    "title": "Algorithm 1",
    "section": "1. Data Processing",
    "text": "1. Data Processing\nWe summarize our data cleaning process below to serve as an example, but the data cleaning needs will be different depending on the particulars of the wastewater surveillance system.\n\ninputs: raw lab values of viral load observations\ndata processing steps:\n\nidentify observations below the level of detection using statistical analysis\nalign all observations to Mondays\ntransform copies per L to a log10 scale\naverage replicates to give one weekly measurement per week per location\nonly use locations where the primary WWTP has at least 85% coverage and observations within 1 month of last date\nensure there is a row for each week, even if the observation is missing\ncreate an indicator of missing values\nremove irrelevant features/variables\n\noutput: a data frame with four variables:\n\ndate\nname of location\nlog10 copies per/L\nmissing data indicator\n\n\n\n# load the cleaned/prepped data\nall_ts_observed &lt;- read.csv(\"Data/synthetic_ww_time_series.csv\")\nall_ts_observed$dates &lt;- as.Date(all_ts_observed$dates)\nhead(all_ts_observed)\n\n       dates           name    value ts_missing  colors\n1 2021-05-24 Lift station A 3.397031      FALSE #44AA99\n2 2021-05-31 Lift station A       NA       TRUE #44AA99\n3 2021-06-07 Lift station A       NA       TRUE #44AA99\n4 2021-06-14 Lift station A       NA       TRUE #44AA99\n5 2021-06-21 Lift station A 4.543146      FALSE #44AA99\n6 2021-06-28 Lift station A 4.356128      FALSE #44AA99",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step2",
    "href": "Algorithm1.html#sec-step2",
    "title": "Algorithm 1",
    "section": "2. Initialize Model",
    "text": "2. Initialize Model\nThe state space model we are fitting needs a certain number of observations to initialize the model. Sometimes this is called the “burnin” period. We have found that about 10 weeks of complete observations are necessary to obtain a good model fit. However, since some of the sampling locations have missing data at the beginning of the series, we set the burnin period to 15 weeks for all series. The code chunk below identifies the dates which will be considered part of the burnin period.\n\nburnin &lt;- 15\ndate_burnin &lt;- all_ts_observed %&gt;% dplyr::filter(name == 'WWTP') %&gt;% dplyr::select(dates) %&gt;% dplyr::nth(burnin) %&gt;% dplyr::pull(dates)\ninit_vals &lt;- c(1, .1) ## set observation variance to be larger than state variance.",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step3",
    "href": "Algorithm1.html#sec-step3",
    "title": "Algorithm 1",
    "section": "3. Online Estimates",
    "text": "3. Online Estimates\nThe model is fit using the KFAS package in R, which can fit any state space model, not just our smoothing spline model. However, KFAS does not use rolling estimation.\nWe have written wrapper functions which fits the smoothing spline state space model and which performs the rolling estimation.\n\n\n\n\n\n\nKFAS_rolling_estimation.r and KFAS_state_space_spline.r\n\n\n\n\n\nState space spline using KFAS::fitSSM . Note the specification of matrices– this is what gives the smoothing spline structure. Different choice of matrices will give a different model structure, e.g. AR(1), ARIMA, etc.\n\nKFAS_state_space_spline &lt;- function(ts_obs, name, ts.missing, ts_dates, init_par){\n  \n## Specify model structure\nA = matrix(c(1,0),1)\nPhi = matrix(c(2,1,-1,0),2)\nmu1 = matrix(0,2) \nP1 = diag(1,2)\nv = matrix(NA) \nR = matrix(c(1,0),2,1)\nw = matrix(NA) \n\n#function for updating the model\nupdate_model &lt;- function(pars, model) {\n  model[\"H\"][1] &lt;- pars[1]\n  model[\"Q\"][1] &lt;- pars[2]\n  model\n}\n\n#check that variances are non-negative\ncheck_model &lt;- function(model) {\n  (model[\"H\"] &gt; 0 && min(model[\"Q\"]) &gt; 0)\n}\n\n# Specify the model\nmod &lt;- KFAS::SSModel(ts_obs ~ -1 +\n                 SSMcustom(Z = A, T = Phi, R = R, Q = w, a1 = mu1, P1 = P1), H = v)\n\n# Fit the model\nfit_mod &lt;- KFAS::fitSSM(mod, inits = init_par, method = \"BFGS\",\n                  updatefn = update_model, checkfn = check_model, hessian=TRUE,\n                  control=list(trace=FALSE,REPORT=1))\n\n## Format for output\nts_len &lt;- length(ts_obs)\nsmoothers &lt;- data.frame(est = KFAS::KFS(fit_mod$model)$alphahat[,1],\n                  lwr = KFAS::KFS(fit_mod$model)$alphahat[,1]- 1.96*sqrt(KFAS::KFS(fit_mod$model)$V[1,1,]),\n                  upr = KFAS::KFS(fit_mod$model)$alphahat[,1]+ 1.96*sqrt(KFAS::KFS(fit_mod$model)$V[1,1,]),\n                  ts_missing = ts.missing,\n                  name = rep(name[1], times = ts_len),\n                  fit = rep(\"smoother\", times = ts_len),\n                  date = ts_dates,\n                  sigv = rep(fit_mod$optim.out$par[1], times = ts_len),\n                  sigw = rep(fit_mod$optim.out$par[2], times = ts_len), \n                  obs = ts_obs, \n                  resid = rstandard(KFAS::KFS(fit_mod$model), type = \"recursive\"),\n                  conv = fit_mod$optim.out$convergence)\n\nfilters &lt;- data.frame(est = KFAS::KFS(fit_mod$model)$att[,1],\n           lwr = KFAS::KFS(fit_mod$model)$att[,1]- 1.96*sqrt(KFAS::KFS(fit_mod$model)$Ptt[1,1,]),\n           upr = KFAS::KFS(fit_mod$model)$att[,1]+ 1.96*sqrt(KFAS::KFS(fit_mod$model)$Ptt[1,1,]),\n           ts_missing = ts.missing,\n           name = rep(name[1], times = ts_len),\n           fit = rep(\"filter\", times = ts_len),\n           date = ts_dates,\n           sigv = rep(fit_mod$optim.out$par[1], times = ts_len),\n           sigw = rep(fit_mod$optim.out$par[2], times = ts_len), \n           obs = ts_obs,\n           resid = rstandard(KFS(fit_mod$model), type = \"recursive\"),\n           conv = fit_mod$optim.out$convergence)\n## combine it all for output.\nkfas_fits_out &lt;- dplyr::bind_rows(smoothers,filters)\nreturn(kfas_fits_out)\n}\n\nRolling estimation code: Note that this calls KFAS_state_space_spline.r multiple times: once for the initialization using the burnin period set above, and the once for each subsequent time point.\n\nKFAS_rolling_estimation &lt;- function(init_vals_roll, \n                               ts_obs_roll,  \n                               ts_name_roll,\n                               dates_roll,\n                               init.par_roll,\n                               ts.missing_roll){\n  \n  ## perform initial fit on \"burnin\" of first init_vals_roll time points \n  fits_rolling&lt;- KFAS_state_space_spline(ts_obs = ts_obs_roll[1:init_vals_roll],\n                                    name = ts_name_roll,                                     \n                                    ts.missing = ts.missing_roll[1:init_vals_roll], \n                                    ts_dates = dates_roll[1:init_vals_roll], \n                                    init_par = init.par_roll)\n\n  \n  # just keep estimates for dates in burnins\n  # smoother need not be kept\n  fits_rolling &lt;- dplyr::filter(fits_rolling, \n                         date == dates_roll[1:init_vals_roll], \n                         fit == \"filter\")\n  \n  \n  # use variance estimates from burnin fit to initialize model for next time point\n  next.par &lt;- c(fits_rolling$sigv[init_vals_roll], fits_rolling$sigw[init_vals_roll])\n  \n  ## perform rolling estimation for each time point\n  for(i in (init_vals_roll +1):length(ts_obs_roll)){\n    # just looking to current time point\n    ts_partial &lt;- ts_obs_roll[1:i]\n\n    # fit the model for the next time point\n    ith_fit &lt;- KFAS_state_space_spline(ts_obs = ts_partial,\n                                      name = ts_name_roll, \n                                      ts.missing = ts.missing_roll[1:i], \n                                      ts_dates = dates_roll[1:i], \n                                      init_par = next.par)\n    # save results of model fit\n    if(exists(\"ith_fit\")){\n      fits_rolling &lt;- rbind(fits_rolling, dplyr::filter(ith_fit, date == dates_roll[i], fit == \"filter\"))\n      # get updated variance estimates for observation and state\n      next.par &lt;- c(ith_fit$sigv[nrow(ith_fit)], ith_fit$sigw[nrow(ith_fit)])\n      ## compute smoother at final time point\n      if(i == length(ts_obs_roll)){\n        fits_rolling &lt;- rbind(fits_rolling, dplyr::filter(ith_fit, fit == \"smoother\"))\n      }\n      rm(ith_fit)\n    }else{ ## I don't know how to error handling, feel free to do a pull request \n      print(rep(\"FAIL\", times = 100))\n    }\n  }\n  ## give the user an update once each series' estimation is complete\n  print(paste(\"Model fit complete: \", ts_name_roll[1])) \n  return(fits_rolling)\n  \n}\n\n\n\n\nWith these functions, we can fit the model.\n\n## source the custom functions\nsource(\"Code/KFAS_state_space_spline.R\")\nsource(\"Code/KFAS_rolling_estimation.R\")\n\n## Rolling estimation for each series\nfits_rolling_KFAS &lt;- all_ts_observed %&gt;%\n  dplyr::group_nest(name, keep = T) %&gt;% \n  tibble::deframe() %&gt;% \n  purrr::map(., ~ {\n      KFAS_rolling_estimation(ts_obs_roll= .x$value,\n                              init_vals_roll = burnin,\n                              ts_name_roll = .x$name,\n                              dates_roll = .x$dates,\n                              ts.missing_roll = .x$ts_missing,\n                              init.par_roll = c(1,.2))\n  })\n\n[1] \"Model fit complete:  Lift station A\"\n[1] \"Model fit complete:  Lift station B\"\n[1] \"Model fit complete:  Lift station C\"\n[1] \"Model fit complete:  Lift station D\"\n[1] \"Model fit complete:  WWTP\"\n\nsave(fits_rolling_KFAS, file = \"Data/fits_rolling_KFAS\")\n\nThe online estimates can be extracted from the output of KFAS_rolling_estimation function by selecting rows with fit == filter (online estimates are called filter estimates in the time series literature, so we preserve this vocabulary in the implementation).\n\nonline_estimates &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;% dplyr::filter(fit == \"filter\")\nhead(online_estimates)\n\n       est      lwr       upr ts_missing           name    fit       date\n1 2.493917 1.483321  3.504513      FALSE Lift station A filter 2021-05-24\n2 4.987834 2.156417  7.819251       TRUE Lift station A filter 2021-05-31\n3 7.481751 2.480884 12.482618       TRUE Lift station A filter 2021-06-07\n4 9.975668 2.752321 17.199016       TRUE Lift station A filter 2021-06-14\n5 4.664197 3.493769  5.834625      FALSE Lift station A filter 2021-06-21\n6 4.709196 3.776564  5.641829      FALSE Lift station A filter 2021-06-28\n       sigv      sigw      obs      resid conv\n1 0.3621268 0.0234552 3.397031  2.9106553    0\n2 0.3621268 0.0234552       NA         NA    0\n3 0.3621268 0.0234552       NA         NA    0\n4 0.3621268 0.0234552       NA         NA    0\n5 0.3621268 0.0234552 4.543146 -1.6277692    0\n6 0.3621268 0.0234552 4.356128 -0.9584143    0\n\n\n\n\n\n\n\n\nOne-step-ahead estimates\n\n\n\n\n\nIn state space models, in addition to obtaining the best estimate of “today’s” state based on data through “today”, the one-step-ahead forecasts can also be obtained: The estimate of tomorrow’s state based on data through today.\nThe KFAS package makes this estimation simple. The function KFAS_state_space_spline.R can be augmented to return the one-step-ahead predictions from the KFS function by accessing the element a, i.e. KFS(fit_mod$model)$a",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step4",
    "href": "Algorithm1.html#sec-step4",
    "title": "Algorithm 1",
    "section": "4. Retrospective Estimates",
    "text": "4. Retrospective Estimates\nThe KFAS_rolling_estimation function also computes the retrospective estimates. They can be extracted by selecting rows with fit == smoother (retrospective estimates are called smoother estimates in the time series literature. This is true is for any state space model, not just those that have a smoothing spline structure).\n\nretro_estimates &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;% dplyr::filter(fit == \"smoother\")\nhead(retro_estimates)\n\n       est      lwr      upr ts_missing           name      fit       date\n1 3.173451 2.804721 3.542180      FALSE Lift station A smoother 2021-05-24\n2 3.543552 3.183237 3.903867       TRUE Lift station A smoother 2021-05-31\n3 3.866474 3.488974 4.243975       TRUE Lift station A smoother 2021-06-07\n4 4.135375 3.789807 4.480943       TRUE Lift station A smoother 2021-06-14\n5 4.343412 4.063163 4.623661      FALSE Lift station A smoother 2021-06-21\n6 4.483741 4.231136 4.736346      FALSE Lift station A smoother 2021-06-28\n        sigv      sigw      obs     resid conv\n1 0.04089099 0.0145985 3.397031  3.329637    0\n2 0.04089099 0.0145985       NA        NA    0\n3 0.04089099 0.0145985       NA        NA    0\n4 0.04089099 0.0145985       NA        NA    0\n5 0.04089099 0.0145985 4.543146 -2.817850    0\n6 0.04089099 0.0145985 4.356128 -1.500047    0",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step5",
    "href": "Algorithm1.html#sec-step5",
    "title": "Algorithm 1",
    "section": "5. Verify model fit",
    "text": "5. Verify model fit\n\nConvergence\nAll fitted models should be checked for convergence. The code below creates visualizations of the convergence code outputted by the optim function, which is the underlying function that actually performs the model fitting. All of our models fit, so the plots below show that all models (one for each time point after burnin) have converged.\n\nVisuals\nSince the models for all the stations do fit well, the below visualizations all look the same. If this were not the case, these visuals are intended to help show if particular intervals of time are failing to converge, which may help with troubleshooting. See the “Troubleshooting model convergence” below for an example of how the plots would look if the estimation of a model failed to converge.\n\nLift station ALift station BLift station CLift station DWWTP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting model convergence (hypothetical example)\n\n\n\n\n\nIf any of the models have not converged, you should not use the output of those models. Here’s an example of what the above plots might look like if the model has not converged for some dates. In the hypothetical example below, the maxit parameter should be increased for the dates with error code 1 and the model corresponding to the date which gave error code 10 should be explored– perhaps there is a lot of missing data or an error was made in the data cleaning step, resulting in extreme values. Note that the L-BFGS-B error codes will only show up if the optimization method is changed to L-BFGS-B.\n\nexample &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;% dplyr::filter(fit == \"filter\" & date &gt;= date_burnin & name == \"WWTP\") \nexample$conv[25:31] &lt;- 1\nexample$conv[45] &lt;- 10\n\n  ggplot2::ggplot(example, aes(x = date, y = conv)) + \n                              ggplot2::geom_point() + \n                              ggplot2::xlim(min(example$date), max(example$date) + 100) +\n                              ggplot2::geom_segment(x = min(example$date), xend = max(example$date), y = 0, yend = 0) + \n                              ggplot2::annotate(\"text\", x = max(example$date) + 50, y = 0, label = \"Converged\")+\n                              ggplot2::geom_segment(x = min(example$date), xend = max(example$date), y = 1, yend = 1) + \n                              ggplot2::annotate(\"text\", x = max(example$date) + 50, y = 2, label = \"maxit reached\")+\n                              ggplot2::geom_segment(x = min(example$date), xend = max(example$date), y = 10, yend = 10) + \n                              ggplot2::annotate(\"text\", x = max(example$date) + 55, y = 10, label = \"Nelder-Mead \\n degeneracy\")+\n                              ggplot2::geom_segment(x = min(example$date), xend = max(example$date), y = 51, yend = 51) + \n                              ggplot2::annotate(\"text\", x = max(example$date) + 60, y = 50, label = \"L-BFGS-B Warning\")+\n                              ggplot2::geom_segment(x = min(example$date), xend = max(example$date), y = 52, yend = 52) + \n                              ggplot2::annotate(\"text\", x = max(example$date) + 65, y = 53, label = \"L-BFGS-B Error\") +\n                            ggplot2::ylab(\"Convergence Code\")  \n\n\n\n\n\n\n\n\n\n\nIf example is the value of online_estimatesfor one location, the dates corresponding to models with convergence issues can be returned using the following code snippet.\n\nexample %&gt;% dplyr::filter(conv &gt;0) %&gt;% dplyr::pull(date, conv)\n\n           1            1            1            1            1            1 \n\"2022-02-14\" \"2022-02-21\" \"2022-02-28\" \"2022-03-07\" \"2022-03-14\" \"2022-03-21\" \n           1           10 \n\"2022-03-28\" \"2022-07-04\" \n\n\n\n\n\n\n\n\nResiduals\nThe visualization of the autocorrelation function (ACF) of the WWTP residuals demonstrates the (standardized) residuals (observed value - filter estimate) of the state space model show a lack of temporal autocorrelation, meaning that the model adequately accounts for the temporal dependence in the WWTP series. For more on using ACF plots see Chapter 2 of Time Series: A Data Analysis Approach Using R.\nIn support of the conclusion from the ACF plot, for all but Lift station B, the Portmanteau Ljung-Box test fails to reject the null hypothesis that the autocorrelations are not significantly different from 0, meaning there is no evidence to suggest the residuals contain temporal dependence– so the time series structure of the wastewater series appears to have been adequately captured.\n\n\nCode\nresid &lt;- fits_rolling_KFAS$`WWTP` %&gt;%  ## just look at the WWTP\n        dplyr::filter(date &gt; date_burnin & fit == \"filter\") %&gt;% ## filter estimates beyond burnin\n        dplyr::mutate(resid = obs-est) %&gt;% dplyr::pull()# calcualte residual\n\n# Ljung-Box test\nLB_test &lt;- Box.test(resid, type = \"Ljung-Box\")\n\n# make acf plot\n#resid %&gt;% acf1(main = \"Autocorrleation plot for Resid = 69th St. Observed - 69th St. Filter\", ylab=\"Autocorrelation\") %&gt;% \n#text(x = 1, y = .35, labels = paste(\"Portmanteau test p-value#: \", round(LB_test$p.value, 4)))\n\n\n\n\n\n\n\n\nResidual plots for all series\n\n\n\n\n\nCode\nTS &lt;- fits_rolling_KFAS %&gt;%\n    dplyr::bind_rows() %&gt;%\n    dplyr::filter(fit == \"filter\" & date &gt;= date_burnin & name == \"WWTP\") \n\nresid_plots &lt;- fits_rolling_KFAS %&gt;%\n                        dplyr::bind_rows() %&gt;%\n                        dplyr::filter(fit == \"filter\" & date &gt;= date_burnin) %&gt;%\n                        dplyr::group_nest(name, keep = T) %&gt;%\n                        tibble::deframe() %&gt;%\n                        purrr::map(., ~{\n\n                          ## impute missing values in .x$resid\n                          resid = zoo::na.approx(.x$resid)\n                          # compute p-value\n                          LB_test &lt;- stats::Box.test(resid, type = \"Ljung-Box\")\n                          # create acf and pacf plots\n                          acf &lt;- forecast::ggAcf(resid, main = paste(.x$name[1], \": ACF\"))\n                          pacf &lt;- forecast::ggPacf(resid, main = paste(TS$name[1], \": PACF\"))\n                          # output single visual for rendering in tabs\n                          acf + pacf + patchwork::plot_annotation(title = paste(\"Portmanteu p-value: \", round(LB_test$p.value, 4)))\n                        })\n\n\n\nVisuals\n\nLift station ALift station BLift station CLift station DWWTP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is lift station B showing significant autocorrelation?\n\n\n\n\n\nThis is cold be due to a linear imputation of almost half of the missing values, which are missing in a big chunk.\n\nall_ts_observed %&gt;% dplyr::group_by(name) %&gt;%dplyr::summarise(missing = sum(ts_missing), total = n(), percent = 100*sum(ts_missing)/n()) %&gt;% dplyr::arrange(desc(percent))\n\n# A tibble: 5 × 4\n  name           missing total percent\n  &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Lift station B      42    95   44.2 \n2 Lift station A      28    95   29.5 \n3 Lift station D      18    95   18.9 \n4 Lift station C       9    95    9.47\n5 WWTP                 4    95    4.21",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step6",
    "href": "Algorithm1.html#sec-step6",
    "title": "Algorithm 1",
    "section": "6. Compare the variances",
    "text": "6. Compare the variances\nAs mentioned above, the parameters we estimate in order to obtain the filters and smoothers above are the state and observation variance terms. The table below provides the estimate of the state and observation variances for the final time point of each series. The plots below visualize the estimates for each in time for each series.\n\n\nCode\npar_est_plots &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;%\n                  dplyr::mutate(colors = rep(c(\"#332288\", \"#AA4499\",\"#44AA99\",\"#88CCEE\", \"#DDCC77\"), each = 95*2)) %&gt;%\n                 dplyr::filter(fit == \"filter\" & date &gt;date_burnin) %&gt;%\n                 tidyr::pivot_longer(cols = c(\"sigv\", \"sigw\"), names_to = \"par\", values_to = \"var_est\") %&gt;%\n                 dplyr::group_nest(name, keep = T) %&gt;%\n                 tibble::deframe() %&gt;% \n                 purrr::map(., ~{\n                    ggplot2::ggplot(.x, aes(x = date, y = var_est, lty = par)) + \n                      ggplot2::geom_line(col = .x$colors[1]) +\n                      ggplot2::theme_minimal() + \n                      ggplot2::scale_linetype_manual(values = c(4,1), labels = c(\"Observation\", \"State\"))+ \n                      ggplot2::labs(linetype = \"Parameter\", x = \"Date\", y = \"Parameter estimate\", title = \"Variance Estimates\")\n                          #missing_dates_lwr &lt;- unique(.x$date[which(.x$ts_missing)])\n                  #missing_dates_upr &lt;- unique(.x$date[which(.x$ts_missing)+1])[1:length(missing_dates_lwr)]\n        \n                  #p+ geom_vline(xintercept = missing_dates_lwr)\n                 }) \n\n## Final variance estimates\nfits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;% dplyr::filter(fit == \"filter\" & date == \"2023-03-13\") %&gt;% dplyr::select(name, sigv, sigw)\n\n\n            name       sigv       sigw\n1 Lift station A 0.04089099 0.01459850\n2 Lift station B 0.11938386 0.01280773\n3 Lift station C 0.26404292 0.01582208\n4 Lift station D 0.25942007 0.01751494\n5           WWTP 0.03758964 0.01055225\n\n\n\n\n\n\n\n\nVariance visualizations\n\n\n\n\n\nVisuals\n\nLift station ALift station BLift station CLift station DWWTP",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm1.html#sec-step7",
    "href": "Algorithm1.html#sec-step7",
    "title": "Algorithm 1",
    "section": "7. Visualize estimates",
    "text": "7. Visualize estimates\n\nFilter estimates\nThe filter estimates are the online estimates— those that use only the data up to the current time point. These are the estimates used in the EWMA charts in Algorithm 2.\n\n\nCode\n#state filter?\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nsource(\"Code/fplot.R\")\nload(\"Data/fits_rolling_KFAS\")\n# plotting the smoothers for all the series\nfilter_plots &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;%\n          dplyr::mutate(colors = rep(c( \"#AA4499\",\"#44AA99\",\"#88CCEE\", \"#DDCC77\", \"#332288\"), each = 95*2)) %&gt;%\n          dplyr::filter(name != \"WWTP\" & fit == \"filter\" & date &gt; date_burnin) %&gt;%\n          dplyr::group_nest(name, keep = T) %&gt;% \n          tibble::deframe() %&gt;%\n          purrr::map(., ~ {\n            plot.dat &lt;- dplyr::bind_rows(dplyr::filter(fits_rolling_KFAS$`WWTP`, fit == \"filter\" & date &gt; date_burnin), .x)\n                        plot.dat$name &lt;- factor(plot.dat$name, levels(factor(plot.dat$name))[2:1])\n            #fplot(f= plot.dat, title_char = \"Comparison of filter estimates for two locations\", line_colors = c( \"#332288\", .x$colors[1]))\n            ggplot2::ggplot(plot.dat, aes(x = date, y = est, color = name, fill = name)) +\n             ggplot2::geom_line(linewidth=2) +\n\n        ggplot2::theme_minimal()+\n\n        ggplot2::geom_ribbon(aes(ymin=lwr,ymax=upr),alpha=.2) +\n\n        ggplot2::scale_color_manual(values = c(\"#332288\", .x$colors[1])) +\n\n        ggplot2::scale_fill_manual(values = c(paste(\"#332288\", \"50\", sep = \"\"), paste(.x$colors[1], \"50\", sep = \"\")), guide = \"none\") +\n                ggplot2::labs(title = \"Comparison of filter estimates for two locations\", x= \"Date\", y = \"Log10 Copies/L-WW\", color = \"\")\n\n          })\n\n\n\nVisualizations\n\nLift station ALift station BLift station CLift station D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmoother estimates\nLike the filter estimates, the smoother estimates have a point estimate and uncertainty estimates visualized. The resulting estimates are indeed smoothed, since the entire series is used to estimate the model at each time point, so the periods of missing data are less pronounced. The 95% confidence bands appear wider for LS that had more missing data. The smoothed estimates are useful for retrospective analyses.\n\n\nCode\nlibrary(ggplot2)\nsource(\"Code/fplot.R\")\n# plotting the smoothers for all the series\nsmoother_plots &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;%\n                  dplyr::mutate(colors = rep(c( \"#AA4499\",\"#44AA99\",\"#88CCEE\", \"#DDCC77\", \"#332288\"), each = 95*2)) %&gt;%\n          dplyr::filter(name != \"WWTP\" & fit == \"smoother\"& date &gt; date_burnin) %&gt;%\n          dplyr::group_nest(name, keep = T) %&gt;% \n          tibble::deframe() %&gt;%\n          purrr::map(., ~ {\n            plot.dat &lt;- dplyr::bind_rows(filter(fits_rolling_KFAS$`WWTP`, fit == \"smoother\" & date &gt; date_burnin), .x)\n            plot.dat$name &lt;- factor(plot.dat$name, levels(factor(plot.dat$name))[2:1])\n            fplot(f= plot.dat, title_char =  \"Comparison of retrospective estimates for two locations\", line_colors = c(\"#332288\",.x$colors[1]))\n          })\n\n\n\nVisualizations\n\nLift station ALift station BLift station CLift station D",
    "crumbs": [
      "Trend Estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Algorithm 1</span>"
    ]
  },
  {
    "objectID": "Algorithm2.html#handle-missing-data-in-ls-series",
    "href": "Algorithm2.html#handle-missing-data-in-ls-series",
    "title": "1. Obtain trend estimates",
    "section": "2. Handle missing data in LS series",
    "text": "2. Handle missing data in LS series\nReplace any missing sub-sewershed observations with WWTP online trend estimate for corresponding date.\n\n# Subset to WWTP\nmu_t &lt;- fits_rolling_KFAS$`WWTP` %&gt;% dplyr::filter(date&gt; \"2021-08-30\"& fit == \"filter\") \n\n\n# Observed LS series and dates\ny_t_all &lt;- fits_rolling_KFAS$`Lift station B` %&gt;% dplyr::filter(date&gt; \"2021-08-30\" & fit == \"filter\")\n\n#Replace missing values in lift station series\ny_t_all[y_t_all$ts_missing, \"obs\"] &lt;- dplyr::left_join(y_t_all[y_t_all$ts_missing,], mu_t, by = \"date\") %&gt;% dplyr::pull(est.y)\n\n# Keep just the series of observations and filled-in missing values\ny_t &lt;- y_t_all %&gt;% dplyr::pull(obs)\n\n# Just the online estimates for WWTP\nmu_t &lt;- fits_rolling_KFAS$`WWTP` %&gt;% dplyr::filter(date&gt; \"2021-08-30\"& fit == \"filter\") %&gt;% dplyr::pull(est)",
    "crumbs": [
      "Detection of Deviations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm 2</span>"
    ]
  },
  {
    "objectID": "Algorithm2.html#create-difference-time-series",
    "href": "Algorithm2.html#create-difference-time-series",
    "title": "1. Obtain trend estimates",
    "section": "3. Create difference time series",
    "text": "3. Create difference time series\nCreate difference time series of sub-sewershed observed copies/liter (\\(\\log10\\)) - WWTP Online Trend Estimate.\n\n## compute the raw differences (numerator of d_tilde)\ndiff &lt;- y_t-mu_t",
    "crumbs": [
      "Detection of Deviations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm 2</span>"
    ]
  },
  {
    "objectID": "Algorithm2.html#standardize-the-difference-series",
    "href": "Algorithm2.html#standardize-the-difference-series",
    "title": "1. Obtain trend estimates",
    "section": "4. Standardize the difference series",
    "text": "4. Standardize the difference series\nCompute the (approximate) standard deviation of the q1. Standardize the difference series by dividing by the standard deviation.\n\nvar_y &lt;- fits_rolling_KFAS$`Lift station B` %&gt;% \n          dplyr::filter(date &gt; \"2021-08-30\" & fit == \"filter\")%&gt;% \n          dplyr::mutate(var_est = sigv^2) %&gt;% select(var_est)\nvar_mu &lt;- fits_rolling_KFAS$`WWTP` %&gt;% \n            dplyr::filter(date &gt; \"2021-08-30\" & fit == \"filter\") %&gt;%\n            dplyr::mutate(var_est = ((upr-est)/2)^2) %&gt;% \n            dplyr::select(var_est)\n\n## compute the estimated covariance (scaled product of variances)\ncor_estimate &lt;- cor(y_t, mu_t, use =\"pairwise.complete.obs\")\ncov_est &lt;- as.numeric(cor_estimate)*sqrt(var_y$var_est)*sqrt(var_mu$var_est)\n\n## compute approximate variance using covariance (this is the correct variance for the numerator of our equation)\nvar_est &lt;- var_y$var_est + var_mu$var_est -2*cov_est\n\n## standardize\nstandardized_diff &lt;- diff/sqrt(var_est)",
    "crumbs": [
      "Detection of Deviations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm 2</span>"
    ]
  },
  {
    "objectID": "Algorithm2.html#construct-ewma-chart",
    "href": "Algorithm2.html#construct-ewma-chart",
    "title": "1. Obtain trend estimates",
    "section": "5. Construct EWMA chart",
    "text": "5. Construct EWMA chart\nConstruct EWMA chart for the standardized difference series.\n\n # compute lag 1 autocorrelation of standardized difference series\n  lag1_est &lt;- acf(standardized_diff, plot=F, na.action = na.pass)$acf[2] ## we could do something fancier\n  \n  # use qcc package to make ewma plot\n  out &lt;- qcc::ewma(standardized_diff, center = 0, sd = 1, \n                   lambda = lag1_est, nsigmas = 3, sizes = 1, plot = F)\n  \n  \n  \n  ## put NAs where we had missing values for either series\n  out$y[is.na(y_t)] &lt;- NA\n  out$data[is.na(y_t),1]&lt;- NA\n  \n  dates &lt;- dplyr::filter(fits_rolling_KFAS$WWTP,\n                         date &gt; \"2021-08-30\" & fit == \"filter\") %&gt;% dplyr::pull(date)\n  \n  # create plot\n  dat &lt;- data.frame(x = dates, \n                    ewma = out$y, \n                    y = out$data[,1], \n                    col = out$x %in% out$violations, \n                    lwr = out$limits[20,1],\n                    upr = out$limits[20,2])\n  obs_dat &lt;- data.frame(x = dat$x, y = out$data[,1], col = \"black\")\n  p &lt;- ggplot2::ggplot(dat, aes(x = x, y = ewma)) + \n    ggplot2::geom_vline(xintercept = NULL, \n               col = \"darkgrey\", \n               lwd = 1) +\n    ggplot2::geom_line()+\n    ggplot2::geom_point(aes(col = dat$col), size = 3) +\n    ggplot2::scale_color_manual(values = c(1,2), label = c(\"No\", \"Yes\"), name = \"Separation?\") +\n    ggnewscale::new_scale_color() +\n    ggplot2::geom_point(data = obs_dat, aes(x = x, y = y, col =col), shape = 3) +\n    ggplot2::scale_color_manual(values = \"black\", label = \"Observed \\nStandardized \\nDifference\", name = \"\") +\n    ggplot2::geom_hline(aes(yintercept = out$limits[20,1]), lty = 2) + \n    ggplot2::geom_hline(aes(yintercept = out$limits[20,2]), lty = 2) +\n    ggplot2::geom_hline(aes(yintercept = 0), lty = 1) + \n    ggplot2::ggtitle(\"Lift station A - WWTP\")+\n    ggplot2::xlab(\"Date\") + ggplot2::ylab(\"Standardized Difference in Series\") +\n    ggplot2::theme_minimal()\n  \n  p",
    "crumbs": [
      "Detection of Deviations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm 2</span>"
    ]
  },
  {
    "objectID": "Algorithm2.html#inspect-ewma-chart",
    "href": "Algorithm2.html#inspect-ewma-chart",
    "title": "1. Obtain trend estimates",
    "section": "6. Inspect EWMA chart",
    "text": "6. Inspect EWMA chart\nInspect EWMA chart for separation. Red dots outside the dotted lines indicate a statistically significant deviation. Note that all the above code can be run by calling the custom ww_ewma.r function.\n\nsource(\"Code/ww_ewma.r\")\n\nmu2 &lt;- fits_rolling_KFAS$`WWTP` %&gt;% dplyr::filter(fit == \"filter\" & date &gt; \"2021-08-30\") \n\newma_plots &lt;- fits_rolling_KFAS %&gt;% dplyr::bind_rows() %&gt;%\n          dplyr::filter(name != \"WWTP\" & fit == \"filter\" & date &gt; \"2021-08-30\") %&gt;%\n          dplyr::group_nest(name, keep = T) %&gt;% \n          tibble::deframe() %&gt;%\n          purrr::map(., ~ {\n               ww_ewma(.x, mu2, paste(.x$name[1], \"-\", mu2$name[1]))\n\n                        }) \n\n\nLift station ALift station BLift station CLift station D",
    "crumbs": [
      "Detection of Deviations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm 2</span>"
    ]
  }
]